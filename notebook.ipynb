{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1eb946558b43db93413d74a9687f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.21 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "# replace 'YOUR_HF_TOKEN_HERE' with your Hugging Face token to authenticate\n",
    "# login(\"YOUR_HF_TOKEN_HERE\")\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer and model with specified quantization configuration\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Calculate and print the memory footprint of the model\n",
    "memory_footprint = model.get_memory_footprint()\n",
    "print(f\"Memory footprint: {memory_footprint / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. How can I assist you today?<|eot_id|>"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# Define the chat messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "]\n",
    "\n",
    "# Prepare model inputs using the chat template\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(model.device)\n",
    "\n",
    "# Initialize the TextIteratorStreamer\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Define generation arguments\n",
    "generation_kwargs = {\n",
    "    **model_inputs,\n",
    "    \"streamer\": streamer,\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "# Start the generation in a separate thread\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# Collect and print the generated text\n",
    "buffer = []\n",
    "for text in streamer:\n",
    "    buffer.append(text)\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 July 2024\n",
      "\n",
      "\n",
      "You have access to the following functions:\n",
      "\n",
      "Use the function 'get_current_weather' to 'Get the current weather in a given location'\n",
      "{\"type\":\"function\",\"function\":{\"name\":\"get_current_weather\",\"description\":\"Get the current weather in a given location\",\"parameters\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\"},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"]}},\"required\":[\"location\"]}}}\n",
      "\n",
      "\n",
      "Think very carefully before calling functions.\n",
      "If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n",
      "\n",
      "Here is an example,\n",
      "<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n",
      "\n",
      "Reminder:\n",
      "- Function calls MUST follow the specified format\n",
      "- Required parameters MUST be specified\n",
      "- Only call one function at a time\n",
      "- Put the entire function call reply on one line\"\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.\n",
      "What's the weather like in Nepal today?\n",
      "\n",
      "{\"location\": \"Kathmandu\", \"temperature\": \"32\", \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "# add syspath to the current directory\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from utils import get_tools_prefix_messages\n",
    "from litserve.specs.openai import Tool, ChatMessage\n",
    "\n",
    "# JSON data\n",
    "tools_json = [\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "          },\n",
    "          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "      },\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "# Convert JSON to Pydantic models\n",
    "tools = [Tool.model_validate(tool) for tool in tools_json]\n",
    "\n",
    "messages_json = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in Nepal today?\"},\n",
    "        {\"role\": \"assistant\",\"content\":\"\",\"tool_calls\": [{'id': 'call_6duDxk', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Kathmandu, NP\", \"unit\": \"celsius\"}'}}]},\n",
    "        {\"role\": \"ipython\",\"tool_call_id\": 'call_deok', \"name\": 'get_current_weather', \"content\": json.dumps({\"location\": \"Kathmandu\", \"temperature\": \"32\", \"unit\": \"celsius\"})  }\n",
    "    ]\n",
    "messages = [ChatMessage.model_validate(message) for message in messages_json]\n",
    "\n",
    "messages = get_tools_prefix_messages(messages, tools)\n",
    "\n",
    "# convert Pydantic models to JSON\n",
    "messages_json = [message.model_dump(exclude_none=True) for message in messages]\n",
    "\n",
    "for message in messages_json:\n",
    "    print(message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PROMPT ==========\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 July 2024\n",
      "\n",
      "\n",
      "You have access to the following functions:\n",
      "\n",
      "Use the function 'get_current_weather' to 'Get the current weather in a given location'\n",
      "{\"type\":\"function\",\"function\":{\"name\":\"get_current_weather\",\"description\":\"Get the current weather in a given location\",\"parameters\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\"},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"]}},\"required\":[\"location\"]}}}\n",
      "\n",
      "\n",
      "Think very carefully before calling functions.\n",
      "If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n",
      "\n",
      "Here is an example,\n",
      "<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n",
      "\n",
      "Reminder:\n",
      "- Function calls MUST follow the specified format\n",
      "- Required parameters MUST be specified\n",
      "- Only call one function at a time\n",
      "- Put the entire function call reply on one line\"\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What's the weather like in Nepal today?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{'id': 'call_6duDxk', 'name': 'get_current_weather', 'arguments': '{\"location\": \"Kathmandu, NP\", \"unit\": \"celsius\"}'}<|eot_id|>\n",
      "<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\"location\": \"Kathmandu\", \"temperature\": \"32\", \"unit\": \"celsius\"}<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "========== PROMPT ==========\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROMPT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mchat_template \u001b[38;5;241m=\u001b[39m chat_template\n\u001b[1;32m     36\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mspecial_tokens_map\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (value \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [value])]\n\u001b[1;32m     37\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     38\u001b[0m     messages_json,\n\u001b[1;32m     39\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from jinja2 import Template\n",
    "from threading import Thread\n",
    "\n",
    "# add syspath to the current directory\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from utils import extract_tool_calls_from_buffer, get_tools_prefix_messages\n",
    "\n",
    "chat_template=\"\"\"\n",
    "{%- for message in messages %}\n",
    "    {%- set prefix = '<|begin_of_text|>' if loop.index0==0 else '' %}\n",
    "    {{- prefix + '<|start_header_id|>'+message['role']+'<|end_header_id|>\\n\\n' -}}\n",
    "    {%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n",
    "        {%- for tool in message['tool_calls'] %}\n",
    "            {%- set tool_json = {'id': tool['id'], 'name': tool['function']['name'], 'arguments': tool['function']['arguments']} %}\n",
    "            {{- tool_json }}\n",
    "        {%- endfor %}\n",
    "        {{- '<|eot_id|>\\n' }}\n",
    "    {%- else %}\n",
    "        {{- message['content'] + '<|eot_id|>\\n' }}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "jinja_template = Template(chat_template.strip())\n",
    "prompt = jinja_template.render(messages=messages_json, add_generation_prompt=True)\n",
    "print(\"=\"*10,\"PROMPT\",\"=\"*10)\n",
    "print(prompt)\n",
    "print(\"=\"*10,\"PROMPT\",\"=\"*10)\n",
    "\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "special_tokens = [item for value in tokenizer.special_tokens_map.values() for item in (value if isinstance(value, list) else [value])]\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    messages_json,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(model.device)\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "  tokenizer,\n",
    "  skip_prompt=True,\n",
    "  skip_special_tokens=False,\n",
    "  clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    **model_inputs,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "buffer = []\n",
    "for text in streamer:\n",
    "    buffer.append(text)\n",
    "    print(text, end=\"\", flush=True)\n",
    "\n",
    "    # check if tool calls\n",
    "    if \"\".join(buffer).startswith(\"<function\"):\n",
    "        tool_calls = extract_tool_calls_from_buffer(buffer) # type: ignore\n",
    "        if tool_calls:\n",
    "          print(\"\\nTOOLS\",tool_calls)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
